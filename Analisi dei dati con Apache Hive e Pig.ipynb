{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analisi dei dati con Apache Hive e Pig\n",
    "\n",
    "Utilizzeremo ``hive`` come linguaggio di query per la creazione del database relativo ai passeggeri del Titanic e per trasformare i dati in modo da consentire a Pig di accedervi correttamente. Il dataset ``csv`` del Titanic ha una serie di campi vuoti in alcune rghe e non è _qualified_ cioè non garantisce che tutti i campi testuali siano racchiusi da apici.\n",
    "\n",
    "Successivamente useremo ``pig`` per eseguire la nostra analisi sul tasso di sopravvivenza per classe e per genere dei passeggeri.\n",
    "\n",
    "## Installazione e configurazione\n",
    "\n",
    "Entrambi i pacchetti possono essere installati scaricandoli dal sito di [Apache Foundation](https://downloads.apache.org/). ``pig`` non richiede particolari configurazioni perché è un client che si connette direttamente alla URL del namenode HDFS [hdfs://localhost:8020]() ovvero [hdfs://localhost:9000]().\n",
    "\n",
    "D'altro canto ``hive`` deve essere configurato per quanto riguarda quantomeno la creazione del metastore e l'accesso al server hive. Assumeremo un metastore implementato come database MySQL.\n",
    "\n",
    "Si consiglia di installare `hive`all'interno della home directory dell'utente `hadoop` proprietario di Apache hadoop.\n",
    "\n",
    "### Creazione del metastore\n",
    "\n",
    "Si creerà undatabase ``metastore`` con utente ``hiveuser/password``.\n",
    "\n",
    "```sql\n",
    "mysql> CREATE DATABASE metastore;\n",
    "mysql> USE metastore;\n",
    "mysql> CREATE USER 'hiveuser'@'localhost' IDENTIFIED BY 'password';\n",
    "mysql> GRANT * ON metastore.* TO 'hiveuser'@'localhost' WITH GRANT OPTION;\n",
    "mysql> FLUSH PRIVILEGES;\n",
    "```\n",
    "\n",
    "### Creazione del file di configurazione ``hive-site.xml``\n",
    "\n",
    "Il file di configurazione dovrà almeno contenere le informazioni per l'accesso al database MySQL che implementa il metastore e le credenziali per il server. \n",
    "\n",
    "Si consiglia di copiare il file template e poi modificarlo:\n",
    "\n",
    "```bash\n",
    "$ export HIVE_HOME=/percorso/di/installazione/hive\n",
    "$ export PATH=$PATH:$HIVE_HOME/bin\n",
    "$ cd $HIVE_HOME/conf\n",
    "$ cp hive-default.xml.template hive-site.xml\n",
    "```\n",
    "\n",
    "Le seguenti informazioni vanno inserite/modificate che riguardano il driver ``jdbc`` per l'accesso al database, le crdenziali del'utent del database e la posizione della directory temporanea in HDFS dove ``hive`` gestirà i suoi dati.\n",
    "\n",
    "```xml\n",
    "<property>\n",
    "  <name>javax.jdo.option.ConnectionURL</name>\n",
    "  <value>jdbc:mysql://localhost/metastore</value>\n",
    "</property>\n",
    "<property>\n",
    "  <name>javax.jdo.option.ConnectionDriverName</name>\n",
    "  <value>com.mysql.cj.jdbc.Driver</value>\n",
    "</property>\n",
    "<property>\n",
    "  <name>javax.jdo.option.ConnectionUserName</name>\n",
    "  <value>hiveuser</value>\n",
    "</property>\n",
    "<property>\n",
    "  <name>javax.jdo.option.ConnectionPassword</name>\n",
    "  <value>password</value>\n",
    "</property>\n",
    "<property>\n",
    "  <name>datanucleus.fixedDatastore</name>\n",
    "  <value>false</value>\n",
    "</property>\n",
    "<property>\n",
    "    <name>hive.exec.local.scratchdir</name>\n",
    "    <value>/tmp/hive</value>\n",
    "    <description>Local scratch space for Hive jobs</description>\n",
    "</property>\n",
    "<property>\n",
    "    <name>hive.downloaded.resources.dir</name>\n",
    "    <value>/tmp/hive</value>\n",
    "    <description>Temporary local directory for added resources in the remote file system.</description>\n",
    "</property>\n",
    "<property>\n",
    "    <name>hive.querylog.location</name>\n",
    "    <value>/tmp/hive</value>\n",
    "    <description>Location of Hive run time structured log file</description>\n",
    "</property>\n",
    "<!-- Accesso al server hive -->\n",
    "<property>\n",
    "    <name>hive.server2.thrift.client.user</name>\n",
    "    <value>pirrone</value>\n",
    "    <description>Username to use against thrift client</description>\n",
    "</property>\n",
    "<property>\n",
    "    <name>hive.server2.thrift.client.password</name>\n",
    "    <value>pirrone</value>\n",
    "    <description>Password to use against thrift client</description>\n",
    "</property>\n",
    "```\n",
    "\n",
    "Successivamente, vanno create le cartelle in HFDS per l'uso di ``hive``:\n",
    "\n",
    "```bash\n",
    "$ hadoop fs -mkdir -p /user/hive/warehouse\n",
    "$ hadoop fs -chmod g+w /user/hive/warehouse\n",
    "$ hadoop fs -mkdir -p /tmp\n",
    "$ hadoop fs -chmod g+w /tmp\n",
    "$ hadoop fs -mkdir -p /tmp/hive\n",
    "$ hadoop fs -chmod 777 /tmp/hive\n",
    "```\n",
    "\n",
    "A questo punto si può inizializzare lo schema del metastore con il comando ``schematool``, essendo certi che punterà al nostro database:\n",
    "\n",
    "```bash\n",
    "$ schematool -dbType mysql -initSchema -userName hiveuser -passWord password \n",
    "```\n",
    "\n",
    "## Connessione a HDFS con ``hive``  o ``beeline``\n",
    "\n",
    "L'accesso tramite la CLI di ``hive`` è immediato:\n",
    "\n",
    "```bash\n",
    "$ hive\n",
    "hive> \n",
    "```\n",
    "\n",
    "Per utilizzare ``beeline``, per altro consigliato, si deve attivare il server perché ci si deve connettere via rete.\n",
    "\n",
    "```bash\n",
    "$ hiveserver2 &\n",
    "$ beeline\n",
    "Beeline version 2.3.7 by Apache Hive\n",
    "beeline>!connect jdbc:hive2://localhost:10000\n",
    "Connecting to jdbc:hive2://localhost:10000\n",
    "Enter username for jdbc:hive2://localhost:10000: pirrone\n",
    "Enter password for jdbc:hive2://localhost:10000: *******\n",
    "Connected to: Apache Hive (version 2.3.7)\n",
    "Driver: Hive JDBC (version 2.3.7)\n",
    "Transaction isolation: TRANSACTION_REPEATABLE_READ\n",
    "0: jdbc:hive2://localhost:10000> \n",
    "````\n",
    "\n",
    "Da questo momento possiamo operare con il linguaggio di query.\n",
    "\n",
    "\n",
    "## Creazione del data set\n",
    "\n",
    "Creiamo lo schema della tabella per ospitare i dati del dataset titanic. Useremo un SERializer/DEserializer per leggere le righe e salteremo la riga delle intestazioni.\n",
    "\n",
    "```sql\n",
    "create table passengers (id int, survived int, class int, name varchar(64), \n",
    "sex varchar(8), age float, sibsp int, parch int, ticket varchar(20), \n",
    "fare float, cabin varchar(20), embarked char(1)) \n",
    "row format serde 'org.apache.hadoop.hive.serde2.OpenCSVSerde' \n",
    "TBLPROPERTIES (\"skip.header.line.count\"=\"1\");\n",
    "````\n",
    "\n",
    "Carichiamo i dati dal file system locale.\n",
    "\n",
    "```sql\n",
    "load data local inpath './Data/titanic.csv' into table passengers;\n",
    "```\n",
    "\n",
    "Creiamo lo shcema tipizzato attribuendo un tipo ai campi e impostando la tabulazione come separatore dei campi.\n",
    "\n",
    "```sql\n",
    "create table passengers_qualified (id int, survived int, class int, \n",
    "name varchar(64), sex varchar(8), age float, sibsp int, parch int, \n",
    "ticket varchar(20), fare float, cabin varchar(20), embarked char(1)) \n",
    "row format delimited fields terminated by '\\t';\n",
    "````\n",
    "\n",
    "Infine carichiamo i dati nel nuovo schema. Questo implicitamente scrive un file ``passengers_qualified`` in HDFS. Questo file verrà caricato in ``pig`` per continuare l'analisi.\n",
    "\n",
    "```sql\n",
    "insert overwrite table passengers_qualified select * from passengers;\n",
    "```\n",
    "\n",
    "\n",
    "## Analisi dei dati in ``pig``\n",
    "\n",
    "Iniziamo registrando una UDF (User Defined Function) codificata in Javascript per il calcolo della media.\n",
    "\n",
    "La funzion si trova sul file ``percentage_pig.js``:\n",
    "\n",
    "```javascript\n",
    "percentage.OutputSchema = \"perc:double\";\n",
    "\n",
    "function percentage(num, total){\n",
    "\treturn num * 100.0 / total;\n",
    "}\n",
    "```\n",
    "\n",
    "L'analogo Python usa i decoratori:\n",
    "\n",
    "```python\n",
    "from pig_util import outputSchema;\n",
    "\n",
    "@outputSchema(\"perc: double\")\n",
    "def percentage(num, total):\n",
    "\treturn num*100/total;\n",
    "```\n",
    "\n",
    "La direttiva di registrazione è:\n",
    "\n",
    "```\n",
    "register './percentage_pig.js' using javascript as myfuncs;\n",
    "```\n",
    "\n",
    "Carichiamo i dati generati da ``hive`` usando la funzione ``PigStorage()`` che gestisce le operazioni SER/DE e impostando lo schema dei dati.\n",
    "\n",
    "```\n",
    "titanic = load 'hdfs://localhost:9099/user/hive/warehouse/titanic.db/passengers_qualified' \n",
    "using PigStorage() as (id:int, survived:int, class:int, \n",
    "name:chararray, sex:chararray,age:int, sibsp:int, parch:int, ticket:chararray, \n",
    "fare:double, cabin:chararray, embarked:chararray);\n",
    "```\n",
    "\n",
    "Raggruppiamo i dati per genere e classe al fine di contare gli imbarcati per ogni tipo. Queste clausole **non eseguono** task YARN sui dati. Agiscono solo sui metadati. Le operazioni di ``dump`` materializzano gli alias generando il piano di esecuzione MapReduce.\n",
    "\n",
    "```\n",
    "embarked = group titanic by (sex,class);\n",
    "\n",
    "embarked_totals = FOREACH embarked GENERATE $0, COUNT(titanic) as embarked:int;\n",
    "```\n",
    "\n",
    "Generiamo l'alias dei sopravvissuti e raggruppiamolo per classe e genere.\n",
    "\n",
    "```\n",
    "survived = filter titanic by survived == 1;\n",
    "\n",
    "survived_grouped = group survived by (sex,class);\n",
    "```\n",
    "\n",
    "Eseguiamo il join sulla chiave ``(sex, class)`` di imbarcati e sopravvissuti\n",
    "\n",
    "```\n",
    "embarked_survived = JOIN survived_grouped BY $0, embarked_totals BY $0;\n",
    "```\n",
    "\n",
    "Generiamo infine i conteggi dei sopravvissuti, degli imbarcati, la percentuale e l'età media a partire dal join.\n",
    "\n",
    "```\n",
    "survived_counts = FOREACH embarked_survived GENERATE $0, COUNT(survived), $3,\n",
    "COUNT(survived)*100.0/$3, AVG(survived.age);\n",
    "```\n",
    "\n",
    "Analogo con la nostra UDF\n",
    "\n",
    "```\n",
    "survived_stats = FOREACH survived_counts GENERATE $0, $1, myfuncs.percentage($1,$2), $4;\n",
    "```\n",
    "\n",
    "Salvataggio in HDFS tramite il SER/DE ``PigStorage()``\n",
    "\n",
    "```\n",
    "store survived_counts into 'hdfs://localhost:9099/user/pirrone/pig/survived_stats' using PigStorage();\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analisi dei dati con Apache Hive e Pig\n",
    "\n",
    "Utilizzeremo ``hive`` come linguaggio di query per la creazione del database relativo ai passeggeri del Titanic e per trasformare i dati in modo da consentire a Pig di accedervi correttamente. Il dataset ``csv`` del Titanic ha una serie di campi vuoti in alcune rghe e non è _qualified_ cioè non garantisce che tutti i campi testuali siano racchiusi da apici.\n",
    "\n",
    "Successivamente useremo ``pig`` per eseguire la nostra analisi sul tasso di sopravvivenza per classe e per genere dei passeggeri.\n",
    "\n",
    "## Installazione e configurazione\n",
    "\n",
    "Entrambi i pacchetti possono essere installati scaricandoli dal sito di [Apache Foundation](https://downloads.apache.org/). ``pig`` non richiede particolari configurazioni perché è un client che si connette direttamente alla URL del namenode HDFS [hdfs://localhost:8020]() ovvero [hdfs://localhost:9000]().\n",
    "\n",
    "D'altro canto ``hive`` deve essere configurato per quanto riguarda quantomeno la creazione del metastore e l'accesso al server hive. Assumeremo un metastore implementato come database MySQL.\n",
    "\n",
    "Si consiglia di installare `hive`all'interno della home directory dell'utente `hadoop` proprietario di Apache hadoop.\n",
    "\n",
    "### Creazione del metastore\n",
    "\n",
    "Si creerà undatabase ``metastore`` con utente ``hiveuser/password``.\n",
    "\n",
    "```sql\n",
    "mysql> CREATE DATABASE metastore;\n",
    "mysql> USE metastore;\n",
    "mysql> CREATE USER 'hiveuser'@'localhost' IDENTIFIED BY 'password';\n",
    "mysql> GRANT * ON metastore.* TO 'hiveuser'@'localhost' WITH GRANT OPTION;\n",
    "mysql> FLUSH PRIVILEGES;\n",
    "```\n",
    "\n",
    "### Creazione del file di configurazione ``hive-site.xml``\n",
    "\n",
    "Il file di configurazione dovrà almeno contenere le informazioni per l'accesso al database MySQL che implementa il metastore e le credenziali per il server. \n",
    "\n",
    "Si consiglia di copiare il file template e poi modificarlo:\n",
    "\n",
    "```bash\n",
    "$ export HIVE_HOME=/percorso/di/installazione/hive\n",
    "$ export PATH=$PATH:$HIVE_HOME/bin\n",
    "$ cd $HIVE_HOME/conf\n",
    "$ cp hive-default.xml.template hive-site.xml\n",
    "```\n",
    "\n",
    "Le seguenti informazioni vanno inserite/modificate che riguardano il driver ``jdbc`` per l'accesso al database, le crdenziali del'utent del database e la posizione della directory temporanea in HDFS dove ``hive`` gestirà i suoi dati.\n",
    "\n",
    "```xml\n",
    "<property>\n",
    "  <name>javax.jdo.option.ConnectionURL</name>\n",
    "  <value>jdbc:mysql://localhost/metastore</value>\n",
    "</property>\n",
    "<property>\n",
    "  <name>javax.jdo.option.ConnectionDriverName</name>\n",
    "  <value>com.mysql.cj.jdbc.Driver</value>\n",
    "</property>\n",
    "<property>\n",
    "  <name>javax.jdo.option.ConnectionUserName</name>\n",
    "  <value>hiveuser</value>\n",
    "</property>\n",
    "<property>\n",
    "  <name>javax.jdo.option.ConnectionPassword</name>\n",
    "  <value>password</value>\n",
    "</property>\n",
    "<property>\n",
    "  <name>datanucleus.fixedDatastore</name>\n",
    "  <value>false</value>\n",
    "</property>\n",
    "<property>\n",
    "    <name>hive.exec.local.scratchdir</name>\n",
    "    <value>/tmp/hive</value>\n",
    "    <description>Local scratch space for Hive jobs</description>\n",
    "</property>\n",
    "<property>\n",
    "    <name>hive.downloaded.resources.dir</name>\n",
    "    <value>/tmp/hive</value>\n",
    "    <description>Temporary local directory for added resources in the remote file system.</description>\n",
    "</property>\n",
    "<property>\n",
    "    <name>hive.querylog.location</name>\n",
    "    <value>/tmp/hive</value>\n",
    "    <description>Location of Hive run time structured log file</description>\n",
    "</property>\n",
    "<!-- Accesso al server hive -->\n",
    "<property>\n",
    "    <name>hive.server2.thrift.client.user</name>\n",
    "    <value>pirrone</value>\n",
    "    <description>Username to use against thrift client</description>\n",
    "</property>\n",
    "<property>\n",
    "    <name>hive.server2.thrift.client.password</name>\n",
    "    <value>pirrone</value>\n",
    "    <description>Password to use against thrift client</description>\n",
    "</property>\n",
    "```\n",
    "\n",
    "Successivamente, vanno create le cartelle in HFDS per l'uso di ``hive``:\n",
    "\n",
    "```bash\n",
    "$ hadoop fs -mkdir -p /user/hive/warehouse\n",
    "$ hadoop fs -chmod g+w /user/hive/warehouse\n",
    "$ hadoop fs -mkdir -p /tmp\n",
    "$ hadoop fs -chmod g+w /tmp\n",
    "$ hadoop fs -mkdir -p /tmp/hive\n",
    "$ hadoop fs -chmod 777 /tmp/hive\n",
    "```\n",
    "\n",
    "A questo punto si può inizializzare lo schema del metastore con il comando ``schematool``, essendo certi che punterà al nostro database:\n",
    "\n",
    "```bash\n",
    "$ schematool -dbType mysql -initSchema -userName hiveuser -passWord password \n",
    "```\n",
    "\n",
    "## Connessione a HDFS con ``hive``  o ``beeline``\n",
    "\n",
    "L'accesso tramite la CLI di ``hive`` è immediato:\n",
    "\n",
    "```bash\n",
    "$ hive\n",
    "hive> \n",
    "```\n",
    "\n",
    "Per utilizzare ``beeline``, per altro consigliato, si deve attivare il server perché ci si deve connettere via rete.\n",
    "\n",
    "```bash\n",
    "$ hiveserver2 &\n",
    "$ beeline\n",
    "Beeline version 2.3.7 by Apache Hive\n",
    "beeline>!connect jdbc:hive2://localhost:10000\n",
    "Connecting to jdbc:hive2://localhost:10000\n",
    "Enter username for jdbc:hive2://localhost:10000: pirrone\n",
    "Enter password for jdbc:hive2://localhost:10000: *******\n",
    "Connected to: Apache Hive (version 2.3.7)\n",
    "Driver: Hive JDBC (version 2.3.7)\n",
    "Transaction isolation: TRANSACTION_REPEATABLE_READ\n",
    "0: jdbc:hive2://localhost:10000> \n",
    "````\n",
    "\n",
    "Da questo momento possiamo operare con il linguaggio di query.\n",
    "\n",
    "\n",
    "## Creazione del data set\n",
    "\n",
    "Creiamo lo schema della tabella per ospitare i dati del dataset titanic. Useremo un SERializer/DEserializer per leggere le righe e salteremo la riga delle intestazioni.\n",
    "\n",
    "```sql\n",
    "create table passengers (id int, survived int, class int, name varchar(64), \n",
    "sex varchar(8), age float, sibsp int, parch int, ticket varchar(20), \n",
    "fare float, cabin varchar(20), embarked char(1)) \n",
    "row format serde 'org.apache.hadoop.hive.serde2.OpenCSVSerde' \n",
    "TBLPROPERTIES (\"skip.header.line.count\"=\"1\");\n",
    "````\n",
    "\n",
    "Carichiamo i dati dal file system locale.\n",
    "\n",
    "```sql\n",
    "load data local inpath './Data/titanic.csv' into table passengers;\n",
    "```\n",
    "\n",
    "Creiamo lo shcema tipizzato attribuendo un tipo ai campi e impostando la tabulazione come separatore dei campi.\n",
    "\n",
    "```sql\n",
    "create table passengers_qualified (id int, survived int, class int, \n",
    "name varchar(64), sex varchar(8), age float, sibsp int, parch int, \n",
    "ticket varchar(20), fare float, cabin varchar(20), embarked char(1)) \n",
    "row format delimited fields terminated by '\\t';\n",
    "````\n",
    "\n",
    "Infine carichiamo i dati nel nuovo schema. Questo implicitamente scrive un file ``passengers_qualified`` in HDFS. Questo file verrà caricato in ``pig`` per continuare l'analisi.\n",
    "\n",
    "```sql\n",
    "insert overwrite table passengers_qualified select * from passengers;\n",
    "```\n",
    "\n",
    "\n",
    "## Analisi dei dati in ``pig``\n",
    "\n",
    "Iniziamo registrando una UDF (User Defined Function) codificata in Javascript per il calcolo della media.\n",
    "\n",
    "La funzion si trova sul file ``percentage_pig.js``:\n",
    "\n",
    "```javascript\n",
    "percentage.OutputSchema = \"perc:double\";\n",
    "\n",
    "function percentage(num, total){\n",
    "\treturn num * 100.0 / total;\n",
    "}\n",
    "```\n",
    "\n",
    "L'analogo Python usa i decoratori:\n",
    "\n",
    "```python\n",
    "from pig_util import outputSchema;\n",
    "\n",
    "@outputSchema(\"perc: double\")\n",
    "def percentage(num, total):\n",
    "\treturn num*100/total;\n",
    "```\n",
    "\n",
    "La direttiva di registrazione è:\n",
    "\n",
    "```\n",
    "register './percentage_pig.js' using javascript as myfuncs;\n",
    "```\n",
    "\n",
    "Carichiamo i dati generati da ``hive`` usando la funzione ``PigStorage()`` che gestisce le operazioni SER/DE e impostando lo schema dei dati.\n",
    "\n",
    "```\n",
    "titanic = load 'hdfs://localhost:9099/user/hive/warehouse/titanic.db/passengers_qualified' \n",
    "using PigStorage() as (id:int, survived:int, class:int, \n",
    "name:chararray, sex:chararray,age:int, sibsp:int, parch:int, ticket:chararray, \n",
    "fare:double, cabin:chararray, embarked:chararray);\n",
    "```\n",
    "\n",
    "Raggruppiamo i dati per genere e classe al fine di contare gli imbarcati per ogni tipo. Queste clausole **non eseguono** task YARN sui dati. Agiscono solo sui metadati. Le operazioni di ``dump`` materializzano gli alias generando il piano di esecuzione MapReduce.\n",
    "\n",
    "```\n",
    "embarked = group titanic by (sex,class);\n",
    "\n",
    "embarked_totals = FOREACH embarked GENERATE $0, COUNT(titanic) as embarked:int;\n",
    "```\n",
    "\n",
    "Generiamo l'alias dei sopravvissuti e raggruppiamolo per classe e genere.\n",
    "\n",
    "```\n",
    "survived = filter titanic by survived == 1;\n",
    "\n",
    "survived_grouped = group survived by (sex,class);\n",
    "```\n",
    "\n",
    "Eseguiamo il join sulla chiave ``(sex, class)`` di imbarcati e sopravvissuti\n",
    "\n",
    "```\n",
    "embarked_survived = JOIN survived_grouped BY $0, embarked_totals BY $0;\n",
    "```\n",
    "\n",
    "Generiamo infine i conteggi dei sopravvissuti, degli imbarcati, la percentuale e l'età media a partire dal join.\n",
    "\n",
    "```\n",
    "survived_counts = FOREACH embarked_survived GENERATE $0, COUNT(survived), $3,\n",
    "COUNT(survived)*100.0/$3, AVG(survived.age);\n",
    "```\n",
    "\n",
    "Analogo con la nostra UDF\n",
    "\n",
    "```\n",
    "survived_stats = FOREACH survived_counts GENERATE $0, $1, myfuncs.percentage($1,$2), $4;\n",
    "```\n",
    "\n",
    "Salvataggio in HDFS tramite il SER/DE ``PigStorage()``\n",
    "\n",
    "```\n",
    "store survived_counts into 'hdfs://localhost:9099/user/pirrone/pig/survived_stats' using PigStorage();\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.3 ('bigdata')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "vscode": {
   "interpreter": {
    "hash": "8689cd57ed0a7d5c2c9c6d41174625a621d3f1ee39f5382a0fe68c3234b001f4"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
